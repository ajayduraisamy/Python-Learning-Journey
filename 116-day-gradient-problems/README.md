# Day 116 â€“ Vanishing and Exploding Gradients

Deep neural networks can fail to train due to
vanishing or exploding gradients during backpropagation.

Covered:
- Why gradients vanish or explode
- Effect of depth and activation functions
- Impact on training stability
- Common mitigation strategies

Understanding gradient flow is essential for
designing deep learning architectures.
