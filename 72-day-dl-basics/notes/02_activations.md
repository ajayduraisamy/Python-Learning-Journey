# Activation Functions

Why activations?
- They add non-linearity
- Allow neural networks to learn complex patterns

Common types:
1. Sigmoid
2. ReLU
3. Tanh
4. Softmax
