# DL Day 16 â€” Multi-Head Attention (MHA)

Includes:
- Theory about why multiple heads
- Full MHA implementation (TensorFlow)
- Demo with random tensors
- Toy example visualizing weights
- Ready for Transformer Encoder (Day 88)

Run demos:
