# Day 118 â€“ Attention Mechanism (Core Intuition)

Attention allows models to focus on relevant parts of the input
instead of compressing everything into a fixed-size vector.

Covered:
- Query, Key, Value intuition
- Why dot-product attention works
- Softmax as relevance weighting
- Why attention scales better than RNNs

This is the foundation of Transformers and LLMs.
