# Weight Initialization

### Xavier / Glorot
Used for tanh, sigmoid activations.

### He Initialization
Used for ReLU-based networks.

Bad initialization causes:
- Vanishing gradients
- Slow training
- Unstable updates
