# Regularization Techniques

### L1 / L2 Regularization
Prevents overfitting by penalizing weights.

### Dropout
Randomly drops neurons → generalization ↑.

### BatchNorm
Stabilizes & speeds up training.

### LayerNorm
Used heavily in Transformers.
