# Day 121 â€“ Positional Embeddings in Transformers

Transformers have no inherent notion of token order.
Positional embeddings inject sequence order information.

Covered:
- Why position information is required
- Absolute positional encodings (sinusoidal)
- Limitations of absolute positions
- Rotary Positional Embeddings (RoPE) intuition

This concept directly impacts context length
and generalization in LLMs.
