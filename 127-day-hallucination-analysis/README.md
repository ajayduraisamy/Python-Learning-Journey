# Day 127 â€“ Hallucination in Large Language Models

Hallucination occurs when an LLM generates
confident but incorrect information.

Covered:
- Why hallucinations happen (training & inference causes)
- Overconfidence and lack of grounding
- Effect of vague prompts
- Engineering strategies to reduce hallucination

Hallucination mitigation is critical for production LLM systems.
